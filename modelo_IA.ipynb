{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 4070 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python train.py --img 640 --batch 16 --epochs 20 --data data.yaml --cfg ./cfg/training/yolov7.yaml --weights yolov7.pt --device 0\n",
    "#!python train.py --img 640 --batch 16 --epochs 10 --data ./path/to/data.yaml --cfg ./cfg/training/yolov7.yaml --weights yolov7.pt --device 0 --save_period 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(weights=['runs/train/exp2/weights/best.pt'], data='data.yaml', batch_size=16, img_size=640, conf_thres=0.001, iou_thres=0.65, task='val', device='0', single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project='runs/test', name='yolov7', exist_ok=False, no_trace=False, v5_metric=False)\n",
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "IDetect.fuse\n",
      " Convert model to Traced-model... \n",
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n",
      "                 all         496        1196       0.849       0.889       0.912       0.796\n",
      "      aluminium-cans         496         406       0.813       0.936       0.925       0.804\n",
      "               glass         496         380       0.912        0.85       0.897       0.811\n",
      "      plastic-bottle         496         410        0.82        0.88       0.915       0.773\n",
      "Speed: 7.7/1.5/9.3 ms inference/NMS/total per 640x640 image at batch-size 16\n",
      "Results saved to runs\\test\\yolov7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOR  v0.1-128-ga207844 torch 2.3.1+cu118 CUDA:0 (NVIDIA GeForce RTX 4070 Ti, 12281.375MB)\n",
      "\n",
      "c:\\Python310\\lib\\site-packages\\torch\\functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3588.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Model Summary: 314 layers, 36503348 parameters, 6194944 gradients, 103.2 GFLOPS\n",
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '..\\valid\\labels.cache' images and labels... 496 found, 0 missing, 2 empty, 0 corrupted: 100%|██████████| 496/496 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '..\\valid\\labels.cache' images and labels... 496 found, 0 missing, 2 empty, 0 corrupted: 100%|██████████| 496/496 [00:00<?, ?it/s]\n",
      "\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:   3%|▎         | 1/31 [00:01<00:38,  1.27s/it]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:   6%|▋         | 2/31 [00:01<00:19,  1.50it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:  10%|▉         | 3/31 [00:01<00:13,  2.07it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:  13%|█▎        | 4/31 [00:02<00:10,  2.60it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:  16%|█▌        | 5/31 [00:02<00:08,  3.22it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:  19%|█▉        | 6/31 [00:02<00:06,  3.73it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:  23%|██▎       | 7/31 [00:02<00:05,  4.21it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:  26%|██▌       | 8/31 [00:02<00:04,  4.60it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:  29%|██▉       | 9/31 [00:02<00:04,  4.67it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:  32%|███▏      | 10/31 [00:03<00:04,  4.94it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:  35%|███▌      | 11/31 [00:03<00:03,  5.26it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:  39%|███▊      | 12/31 [00:03<00:03,  5.33it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:  42%|████▏     | 13/31 [00:03<00:03,  5.45it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:  45%|████▌     | 14/31 [00:03<00:03,  5.57it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:  48%|████▊     | 15/31 [00:03<00:02,  5.65it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:  52%|█████▏    | 16/31 [00:04<00:02,  5.65it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:  55%|█████▍    | 17/31 [00:04<00:02,  5.41it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:  58%|█████▊    | 18/31 [00:04<00:02,  4.68it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:  61%|██████▏   | 19/31 [00:04<00:02,  4.24it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:  65%|██████▍   | 20/31 [00:05<00:02,  4.20it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:  68%|██████▊   | 21/31 [00:05<00:02,  4.31it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:  71%|███████   | 22/31 [00:05<00:02,  4.23it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:  74%|███████▍  | 23/31 [00:05<00:01,  4.19it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:  77%|███████▋  | 24/31 [00:06<00:01,  4.08it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:  81%|████████  | 25/31 [00:06<00:01,  4.13it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:  84%|████████▍ | 26/31 [00:06<00:01,  4.14it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:  87%|████████▋ | 27/31 [00:06<00:01,  3.97it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:  90%|█████████ | 28/31 [00:07<00:00,  3.95it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:  94%|█████████▎| 29/31 [00:07<00:00,  4.05it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:  97%|█████████▋| 30/31 [00:07<00:00,  4.00it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 31/31 [00:07<00:00,  3.81it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 31/31 [00:07<00:00,  3.92it/s]\n"
     ]
    }
   ],
   "source": [
    "!python test.py --data data.yaml --img 640 --batch 16 --conf 0.001 --iou 0.65 --device 0 --weights runs/train/exp2/weights/best.pt --name yolov7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "IDetect.fuse\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from models.experimental import attempt_load\n",
    "from utils.general import non_max_suppression, scale_coords\n",
    "import os\n",
    "\n",
    "model_path = 'runs/train/exp2/weights/best.pt'\n",
    "data_yaml_path = 'data.yaml'\n",
    "\n",
    "image_path = 'C:/Users/elvis/Downloads/prueba.png'\n",
    "\n",
    "img_cv2 = cv2.imread(image_path)\n",
    "if img_cv2 is None:\n",
    "    raise FileNotFoundError(f'No se pudo encontrar la imagen en la ruta: {image_path}')\n",
    "\n",
    "img_resized = cv2.resize(img_cv2, (640, 640))\n",
    "img_resized = img_resized / 255.0\n",
    "img_resized = img_resized.transpose(2, 0, 1)\n",
    "img_resized = np.expand_dims(img_resized, axis=0)\n",
    "img_resized = torch.tensor(img_resized, dtype=torch.float)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = attempt_load(model_path, map_location=device)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    img_resized = img_resized.to(device)\n",
    "    pred = model(img_resized)[0]\n",
    "    pred = non_max_suppression(pred, 0.25, 0.45)\n",
    "\n",
    "\n",
    "for det in pred:\n",
    "    if len(det):\n",
    "        det[:, :4] = scale_coords(img_resized.shape[2:], det[:, :4], img_cv2.shape).round()\n",
    "        for *xyxy, conf, cls in det:\n",
    "            label = f'{model.names[int(cls)]} {conf:.2f}'\n",
    "            xyxy = [int(x) for x in xyxy]\n",
    "            cv2.rectangle(img_cv2, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), (0, 255, 0), 2)\n",
    "            cv2.putText(img_cv2, label, (xyxy[0], xyxy[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "output_folder = 'runs/img_prueba'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "output_image_path = os.path.join(output_folder, 'detected_image_3.jpg')\n",
    "cv2.imwrite(output_image_path, img_cv2)\n",
    "cv2.imshow('Detections', img_cv2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "IDetect.fuse\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from models.experimental import attempt_load\n",
    "from utils.general import non_max_suppression, scale_coords\n",
    "import os\n",
    "\n",
    "model_path = 'runs/train/exp2/weights/best.pt'\n",
    "data_yaml_path = 'data.yaml'\n",
    "\n",
    "# URL de la imagen de internet\n",
    "image_url = 'https://pic.52112.com/180713/JPG-180713_870/whwK6ujFSZ_small.jpg'\n",
    "\n",
    "response = requests.get(image_url)\n",
    "img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "img_cv2 = np.array(img)\n",
    "img_cv2 = img_cv2[:, :, ::-1].copy()  # Convertir de RGB a BGR\n",
    "\n",
    "img_resized = cv2.resize(img_cv2, (640, 640))\n",
    "img_resized = img_resized / 255.0\n",
    "img_resized = img_resized.transpose(2, 0, 1)\n",
    "img_resized = np.expand_dims(img_resized, axis=0)\n",
    "img_resized = torch.tensor(img_resized, dtype=torch.float)\n",
    "\n",
    "# Cargar el modelo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = attempt_load(model_path, map_location=device)\n",
    "model = model.to(device)\n",
    "\n",
    "#\n",
    "# Realizar la detección\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    img_resized = img_resized.to(device)\n",
    "    pred = model(img_resized)[0]\n",
    "    pred = non_max_suppression(pred, 0.25, 0.45)\n",
    "\n",
    "for det in pred:\n",
    "    if len(det):\n",
    "        det[:, :4] = scale_coords(img_resized.shape[2:], det[:, :4], img_cv2.shape).round()\n",
    "        for *xyxy, conf, cls in det:\n",
    "            label = f'{model.names[int(cls)]} {conf:.2f}'\n",
    "            xyxy = [int(x) for x in xyxy]\n",
    "            cv2.rectangle(img_cv2, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), (0, 255, 0), 2)\n",
    "            cv2.putText(img_cv2, label, (xyxy[0], xyxy[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "output_folder = 'runs/img_prueba'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "output_image_path = os.path.join(output_folder, 'detected_image_2.jpg')\n",
    "cv2.imwrite(output_image_path, img_cv2)\n",
    "cv2.imshow('Detections', img_cv2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "IDetect.fuse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\torch\\functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3588.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from models.experimental import attempt_load\n",
    "from utils.general import non_max_suppression, scale_coords\n",
    "import os\n",
    "\n",
    "model_path = 'runs/train/exp2/weights/best.pt'\n",
    "video_path = 'C:/modelo/yolov7/runs/videos_pruebas/prueba_3.mp4'\n",
    "output_path = 'C:/modelo/yolov7/runs/videos_pruebas/procesado_prueba_3.mp4'\n",
    "\n",
    "# Cargar el modelo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = attempt_load(model_path, map_location=device)\n",
    "model = model.to(device)\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter(output_path, fourcc, 20.0, (frame_width, frame_height))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img_resized = cv2.resize(frame, (640, 640))\n",
    "    img_resized = img_resized / 255.0\n",
    "    img_resized = img_resized.transpose(2, 0, 1)\n",
    "    img_resized = np.expand_dims(img_resized, axis=0)\n",
    "    img_resized = torch.tensor(img_resized, dtype=torch.float).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(img_resized)[0]\n",
    "        pred = non_max_suppression(pred, 0.25, 0.45)\n",
    "\n",
    "    for det in pred:\n",
    "        if len(det):\n",
    "            det[:, :4] = scale_coords(img_resized.shape[2:], det[:, :4], frame.shape).round()\n",
    "            for *xyxy, conf, cls in det:\n",
    "                label = f'{model.names[int(cls)]} {conf:.2f}'\n",
    "                xyxy = [int(x) for x in xyxy]\n",
    "                cv2.rectangle(frame, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, label, (xyxy[0], xyxy[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    out.write(frame)\n",
    "    cv2.imshow('Frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "#cerrar\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
